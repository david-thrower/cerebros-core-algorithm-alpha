# -*- coding: utf-8 -*-
"""phishing-email-detection-gpt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10KKTHjBkdfKBpT9OLIj2eZs533BuCS6h

## GPT2 + Cerebros for Phishing email detection

Initialization
"""

import tensorflow as tf
import tensorflow_text
from keras_nlp.models import GPT2Tokenizer, GPT2Preprocessor, GPT2Backbone
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Flatten
import pandas as pd
import numpy as np
from cerebros.simplecerebrosrandomsearch.simple_cerebros_random_search\
    import SimpleCerebrosRandomSearch
import pendulum
from cerebros.units.units import DenseUnit
from cerebros.denseautomlstructuralcomponent.dense_automl_structural_component\
    import zero_7_exp_decay, zero_95_exp_decay, simple_sigmoid
from ast import literal_eval
import time


#
# Load the email data
#
df = pd.read_csv("Phishing_Email.csv")
#
# Get the rows where 'Email Text' is a string, remove everything else
#
df = df[df['Email Text'].apply(lambda x: isinstance(x, str))]
#
# Reset the index
#
df.reset_index(drop=True, inplace=True)

#
# Binary label for email type: positive type is "phishing"
#
label_mapping = {"Safe Email": 0, "Phishing Email": 1}
df["Binary Label"] = df["Email Type"].map(label_mapping)
#
# Data and labels ready
#
X = df["Email Text"].to_numpy()
y = df["Binary Label"].to_numpy()
#
# Shuffle the data
#
X, y = shuffle(X, y)

# Train / test split : we give 85% of the data for *testing*
X_train, X_test, y_train, y_test = \
train_test_split(X, y, test_size=0.85, shuffle=False)

#
# Tensors for training data and labels
#

# Training data for baseline model
baseline_train_x = tf.constant(X_train)
baseline_train_y = tf.constant(y_train, dtype=tf.int8)

# Packaged for Cerebros (multimodal, takes inputs as a list)
training_x   = [baseline_train_x]
train_labels = [baseline_train_y]

#
# Input and output shapes
#
INPUT_SHAPES  = [()]
OUTPUT_SHAPES = [1]

"""### A custom GPT2 encoder layer for text embedding"""

class GPT2Layer(tf.keras.layers.Layer):

    def __init__(self, max_seq_length, **kwargs):
        #
        super(GPT2Layer, self).__init__(**kwargs)
        #
        # Load the GPT2 tokenizer, preprocessor and model
        self.tokenizer = GPT2Tokenizer.from_preset("gpt2_base_en")
        self.preprocessor = GPT2Preprocessor(self.tokenizer,
                                             sequence_length=max_seq_length)
        self.encoder   = GPT2Backbone.from_preset("gpt2_base_en")
        #
        # Set whether the GPT2 model's layers are trainable
        #self.encoder.trainable = False
        for layer in self.encoder.layers:
            layer.trainable = True
        #
        # self.encoder.layers[-2].trainable = True
        #
        # Set the maximum sequence length for tokenization
        self.max_seq_length = max_seq_length

    def call(self, inputs):
        #
        # Output the GPT2 embedding
        prep = self.preprocessor([inputs])
        embedding  = self.encoder(prep)
        avg_pool = tf.reduce_mean(embedding, axis=1)
        #
        return avg_pool

    def get_config(self):
        #
        config = super(GPT2Layer, self).get_config()
        config.update({'max_seq_length': self.max_seq_length})
        #
        return config

    @classmethod
    def from_config(cls, config):
        #
        return cls(max_seq_length=config['max_seq_length'])

# GPT2 configurables
max_seq_length = 96

# GPT Baseline Model
input_layer = Input(shape=(), dtype=tf.string)
gpt2_layer = GPT2Layer(max_seq_length)(input_layer)
#output = Flatten()(gpt2_layer)
binary_output = tf.keras.layers.Dense(1, activation='sigmoid')(gpt2_layer)

gpt_baseline_model = Model(inputs=input_layer, outputs=binary_output)


gpt_baseline_model.compile(
    optimizer=Adam(learning_rate=1e-4),  # Small LR since we're fine-tuning GPT
    loss='binary_crossentropy',
    # metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    metrics=[tf.keras.metrics.BinaryAccuracy(), 
         tf.keras.metrics.Precision(), 
         tf.keras.metrics.Recall()]
)

gpt_t0 = time.time()

print(gpt_baseline_model.summary())
"""
history = gpt_baseline_model.fit(
    x=X_train,  # Input data
    y=y_train,  # Labels
    epochs=4,  # Number of training iterations
    batch_size=16,  # Batch size small due to GPU memory constraints
    validation_split=0.2,  # Hold out 20% of training data for validation
    shuffle=True,  # Shuffle data at each epoch
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=3,
            restore_best_weights=True,
            min_delta=0.001
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=2,
            min_lr=1e-6
        )
    ]
)

gpt_t1 = time.time()
gpt_time_on_one_model_min =  (gpt_t1 - gpt_t0) / 60

hy_df = pd.DataFrame(history.history)
print(hy_df)
"""

### Cerebros model:

# TokenizerLayer (Optimized for Cerebros)
class TokenizerLayer(tf.keras.layers.Layer):
    def __init__(self, max_seq_length, **kwargs):
        super(TokenizerLayer, self).__init__(**kwargs)
        self.tokenizer = GPT2Tokenizer.from_preset("gpt2_base_en")
        self.preprocessor =\
                GPT2Preprocessor(self.tokenizer,
                                 sequence_length=max_seq_length)
        self.max_seq_length = max_seq_length

    def call(self, inputs):
        prep = self.preprocessor([inputs])
        return prep['token_ids']

    def get_config(self):
        config = super(TokenizerLayer, self).get_config()
        config.update({'max_seq_length': self.max_seq_length})
        return config

    @classmethod
    def from_config(cls, config):
      return cls(**config)


# Cerebros Base Model (Optimized)

inp = tf.keras.layers.Input(shape=(), dtype=tf.string)
gp2_tokenizer = TokenizerLayer(max_seq_length=max_seq_length)
VOCABULARY_SIZE = gp2_tokenizer.tokenizer.vocabulary_size()
tokens = gp2_tokenizer(inp)

embedded = tf.keras.layers.Embedding(
        input_dim=VOCABULARY_SIZE,
        output_dim=32,  # Increased embedding dimension
        input_length=max_seq_length,
        mask_zero=True)(tokens)


gru_output =\
    tf.keras.layers.GRU(64,
                        return_sequences=True,
                        dropout=0.3)(embedded)

flattened =\
        tf.keras.layers.Flatten()(gru_output)
dropout =\
        tf.keras.layers.Dropout(0.5)(flattened)

cerebros_base_model =\
        Model(inputs=inp, outputs=dropout)

# Cerebros Search (Optimized Parameters)

# Optimized Cerebros configurables
activation = 'relu'  # 'relu' is often faster and can prevent vanishing gradients
max_seq_length = 1024       # Reduced max_seq_length
epochs = 10              # Reduced epochs for faster search
batch_size = 32           # Increased batch size
minimum_levels = 2
maximum_levels = 3        # Reduced max levels
minimum_units_per_level = 4
maximum_units_per_level = 8
minimum_neurons_per_unit = 8  # Increased min neurons
maximum_neurons_per_unit = 32 # Increased max neurons
moities_to_try = 5         # Reduced for faster search
tries_per_moity = 1
learning_rate = 0.0001    # Adjusted learning rate
cerebros_base_model = create_cerebros_base_model(max_seq_length)

TIME = pendulum.now(tz='America/New_York').__str__()[:16].replace('T', '_').replace(':', '_').replace('-', '_')
PROJECT_NAME = f'_cerebros_auto_ml_phishing_email_test'
meta_trial_number = 42

cerebros_automl =\
        SimpleCerebrosRandomSearch(
            unit_type=DenseUnit,
            input_shapes=input_shapes,
            output_shapes=output_shapes,
            training_data=training_x,
            labels=train_labels,
            validation_split=0.3,   # Reduced validation split
            direction='maximize',
            metric_to_rank_by="val_binary_accuracy",
            minimum_levels=minimum_levels,
            maximum_levels=maximum_levels,
            minimum_units_per_level=minimum_units_per_level,
            maximum_units_per_level=maximum_units_per_level,
            minimum_neurons_per_unit=minimum_neurons_per_unit,
            maximum_neurons_per_unit=maximum_neurons_per_unit,
            activation=activation,
            final_activation='sigmoid',
            number_of_architecture_moities_to_try=moities_to_try,
            number_of_tries_per_architecture_moity=tries_per_moity,
            minimum_skip_connection_depth=1,
            maximum_skip_connection_depth=3,   # Reduced max skip connection depth
            predecessor_level_connection_affinity_factor_first=50,  # Simplified
            predecessor_level_connection_affinity_factor_first_rounding_rule='ceil',
            predecessor_level_connection_affinity_factor_main=0.3, # Simplified
            predecessor_level_connection_affinity_factor_main_rounding_rule='ceil',
            predecessor_level_connection_affinity_factor_decay_main=zero_7_exp_decay,
            seed=8675309,
            max_consecutive_lateral_connections=4,  # Reduced
            gate_after_n_lateral_connections=3,
            gate_activation_function=simple_sigmoid,
            p_lateral_connection=0.4,  # Simplified
            p_lateral_connection_decay=zero_95_exp_decay,
            num_lateral_connection_tries_per_unit=5,   # Reduced
            learning_rate=learning_rate,
            loss=tf.keras.losses.BinaryCrossentropy(),  # Use BinaryCrossentropy directly
            metrics=[tf.keras.metrics.BinaryAccuracy(),
                     tf.keras.metrics.Precision(),
                     tf.keras.metrics.Recall()],
            epochs=epochs,
            project_name=f"_meta_",
            model_graphs='model_graphs',
            batch_size=batch_size,
            meta_trial_number=meta_trial_number,
            base_models=[cerebros_base_model],
            train_data_dtype=tf.string)

cerebros_t0 = time.time()
result = cerebros_automl.run_random_search()
cerebros_t1 = time.time()
cerebros_time_all_models_min = (cerebros_t1 - cerebros_t0) / 60
models_tried = moities_to_try * tries_per_moity
cerebros_time_per_model = cerebros_time_all_models_min / models_tried

print(f"Cerebros trained {models_tried} models in {cerebros_time_all_models_min:.2f} min. Average time per model: {cerebros_time_per_model:.2f} min.")
print(f'Cerebros best accuracy achieved: {result["best_score"]}')
print(f'Validation set accuracy: {result["best_score"]}')
return cerebros_automl


# Evaluate the best model (Corrected Evaluation)
best_model_found = tf.keras.models.load_model(cerebros_automl.best_model_path, custom_objects={'TokenizerLayer': TokenizerLayer})  # Use correct custom object
print('Evaluating on the test dataset:')
best_model_found.evaluate(np.array(X_test), np.array(y_test)) # ensure X_test and y_test are numpy arrays

