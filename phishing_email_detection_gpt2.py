
# -*- coding: utf-8 -*-
"""phishing-email-detection-gpt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10KKTHjBkdfKBpT9OLIj2eZs533BuCS6h

## GPT2 + Cerebros for Phishing email detection

Initialization
"""

import tensorflow as tf
import tensorflow_text
from keras_nlp.models import GPT2Tokenizer, GPT2Preprocessor, GPT2Backbone
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Flatten
import pandas as pd
import numpy as np
from cerebros.simplecerebrosrandomsearch.simple_cerebros_random_search\
    import SimpleCerebrosRandomSearch
import pendulum
from cerebros.units.units import DenseUnit
from cerebros.denseautomlstructuralcomponent.dense_automl_structural_component\
    import zero_7_exp_decay, zero_95_exp_decay, simple_sigmoid
from ast import literal_eval

#
# Load the email data
#
df = pd.read_csv("Phishing_Email.csv")
#
# Get the rows where 'Email Text' is a string, remove everything else
#
df = df[df['Email Text'].apply(lambda x: isinstance(x, str))]
#
# Reset the index
#
df.reset_index(drop=True, inplace=True)

#
# Binary label for email type: positive type is "phishing"
#
label_mapping = {"Safe Email": 0, "Phishing Email": 1}
df["Binary Label"] = df["Email Type"].map(label_mapping)
#
# Data and labels ready
#
X = df["Email Text"].to_numpy()
y = df["Binary Label"].to_numpy()
#
# Shuffle the data
#
X, y = shuffle(X, y)

# Train / test split : we give 85% of the data for *testing*
X_train, X_test, y_train, y_test = \
train_test_split(X, y, test_size=0.85, shuffle=False)

#
# Tensors for training data and labels
#
training_x   = [tf.constant(X_train)]
train_labels = [tf.constant(y_train)]
#
# Input and output shapes
#
INPUT_SHAPES  = [()]
OUTPUT_SHAPES = [1]

# A custom layer for scaling that punished values near 0
# Punitively and returns values in around the same range 
# as the right tail of the original data, but makes it symetrical  
class IdentitySoftSignEmbedding(tf.keras.layers.Layer):
    def __init__(self,
                 input_length=900,
                 input_dropout=0.5,
                 output_len=900,
                 output_dropout=0.5,
                 scale_factor="identity",
                 **kwargs):
        super(IdentitySoftSignEmbedding, self).__init__(**kwargs)
        self.input_length = input_length
        self.input_dropout = input_dropout
        self.output_len = output_len
        self.output_dropout = output_dropout
        self.scale_factor = scale_factor
        self.initialized = False

    def call(self, inputs, initial_weights=None):
        if not self.initialized:
            if initial_weights is None:
                raise ValueError("Initial weights must be provided to initialize the layer.")
            else:
                self.set_weights(initial_weights)
                self.initialized = True

        # Cast the input tensor to tf.float32
        float_inputs = tf.cast(inputs, tf.float32)

        # Compute the maximum value for each sample along the first axis (batch dimension)
        if self.scale_factor == "identity":
            max_values = tf.cast(
                tf.math.reduce_max(
                    float_inputs,
                    axis=-1,
                    keepdims=False),
                tf.float32)
        elif isinstance(self.scale_factor, (int, float)):
            max_values = tf.constant([self.scale_factor] * float_inputs.shape[0], 
                                     dtype=tf.float32)
        else:
            raise ValueError("Scale factor must be set to "
                             "either the string 'identity' "
                             "or an int or float value.")

        # Apply the softsign activation to the batch
        # dropout_first = tf.keras.layers.Dropout(self.input_dropout)()
        dense_inputs = tf.keras.layers.Dense(self.input_length)(float_inputs)
        dropout_out = tf.keras.layers.Dropout(self.output_dropout)(dense_inputs)
        dense_outputs = tf.keras.layers.Dense(self.output_len)(dropout_out)
        batch_through_softsign = tf.keras.activations.softsign(dense_outputs)

        # Multiply the result by the maximum value calculated earlier
        output = batch_through_softsign * max_values
        return output

    def build(self, initial_shape):
        super(IdentitySoftSignEmbedding, self).build(initial_shape)
        self.initialized = True

    def set_weights(self, weights):
        assert len(weights) == self.count_weights(), "Invalid weight count."
        super(IdentitySoftSignEmbedding, self).set_weights(weights)

    def count_weights(self):
        return sum(self.unit_for_size(u).count_weights() for u in self._units)


"""### A custom GPT2 encoder layer for text tokenization"""

class GPT2Layer(tf.keras.layers.Layer):

    def __init__(self, max_seq_length, **kwargs):
        #
        super(GPT2Layer, self).__init__(**kwargs)
        #
        # Load the GPT2 tokenizer, preprocessor and model
        self.tokenizer = GPT2Tokenizer.from_preset("gpt2_extra_large_en") # "gpt2_base_en"
        self.preprocessor = GPT2Preprocessor(self.tokenizer,
                                             sequence_length=max_seq_length)
        # self.encoder   = GPT2Backbone.from_preset("gpt2_base_en")
        #
        # Set whether the GPT2 model's layers are trainable
        # self.encoder.trainable = False
        # for layer in self.encoder.layers:
        #     layer.trainable = False
        #
        # self.encoder.layers[-2].trainable = True
        #
        # Set the maximum sequence length for tokenization
        self.max_seq_length = max_seq_length

    def call(self, inputs):
        #
        # Output the GPT2 embedding
        prep = self.preprocessor([inputs])
        # embedding  = self.encoder(prep)
        # avg_pool = tf.reduce_mean(embedding, axis=1)
        #
        return prep['token_ids']

    def get_config(self):
        #
        config = super(GPT2Layer, self).get_config()
        config.update({'max_seq_length': self.max_seq_length})
        #
        return config

    @classmethod
    def from_config(cls, config):
        #
        return cls(max_seq_length=config['max_seq_length'])

# GPT2 configurables

max_seq_length = 900

inp = tf.keras.layers.Input(shape=(), dtype=tf.string)
gp2 = GPT2Layer(max_seq_length=max_seq_length)
VOCABULARY_SIZE = gp2.tokenizer.vocabulary_size()
tokens = gp2(inp)




embedded = IdentitySoftSignEmbedding(                 
        input_length=max_seq_length,
        input_dropout=0.5,
        output_len=900,
        output_dropout=0.5,
        scale_factor="identity")(tokens)
# embedded =\
#     tf.keras.layers.Embedding(
#         input_dim=VOCABULARY_SIZE,
#         output_dim=15,
#         input_length=max_seq_length,
#         mask_zero=True)(tokens)
# flattened = tf.keras.layers.Flatten()(embedded)
# dropout_embedded = tf.keras.layers.Dropout(0.3)(flattened)
# dense = tf.keras.layers.Dense(max_seq_length, activation=None)(dropout_embedded)
# soft_scaled = IdentitySoftSign(
#     scale_factor=VOCABULARY_SIZE)(dense)


tokenized_embedded_model=\
    tf.keras.Model(
        inputs=inp,
        outputs=embedded)

print(f"VOCABULARY_SIZE: {VOCABULARY_SIZE}")



"""### Cerebros search for the best model"""

#
# Cerebros configurables
#

# Best so far: original params


activation = 'randomize'
predecessor_level_connection_affinity_factor_first = 49.9999
predecessor_level_connection_affinity_factor_main = 0.31456
max_consecutive_lateral_connections = 22
p_lateral_connection = 0.39256
num_lateral_connection_tries_per_unit = 10
learning_rate = 0.0000511065 # original 0.0000511065
epochs = 15  # [1, 100]
batch_size = 20 # Original 20

minimum_levels = 2
maximum_levels = 4 # [3,7]

minimum_units_per_level = 4
maximum_units_per_level = 8

minimum_neurons_per_unit = 1
maximum_neurons_per_unit = 5  # [2,20]


#
# Logging
#
TIME = pendulum.now(tz='America/New_York').__str__()[:16]\
    .replace('T', '_')\
    .replace(':', '_')\
    .replace('-', '_')
PROJECT_NAME = f'{TIME}_cerebros_auto_ml_phishing_email_test'

meta_trial_number = 42 # irrelevant unless in distributed training

cerebros_automl = SimpleCerebrosRandomSearch(
    unit_type=DenseUnit,
    input_shapes=INPUT_SHAPES,
    output_shapes=OUTPUT_SHAPES,
    training_data=training_x,
    labels=train_labels,
    validation_split=0.35,
    direction='maximize',
    metric_to_rank_by="val_binary_accuracy",
    minimum_levels=minimum_levels,
    maximum_levels=maximum_levels,
    minimum_units_per_level=minimum_units_per_level,
    maximum_units_per_level=maximum_units_per_level,
    minimum_neurons_per_unit=minimum_neurons_per_unit,
    maximum_neurons_per_unit=maximum_neurons_per_unit,
    activation=activation,
    final_activation='sigmoid',
    number_of_architecture_moities_to_try=5,
    number_of_tries_per_architecture_moity=1,
    minimum_skip_connection_depth=1,
    maximum_skip_connection_depth=7,
    predecessor_level_connection_affinity_factor_first=predecessor_level_connection_affinity_factor_first,
    predecessor_level_connection_affinity_factor_first_rounding_rule='ceil',
    predecessor_level_connection_affinity_factor_main=predecessor_level_connection_affinity_factor_main,
    predecessor_level_connection_affinity_factor_main_rounding_rule='ceil',
    predecessor_level_connection_affinity_factor_decay_main=zero_7_exp_decay,
    seed=8675309,
    max_consecutive_lateral_connections=max_consecutive_lateral_connections,
    gate_after_n_lateral_connections=3,
    gate_activation_function=simple_sigmoid,
    p_lateral_connection=p_lateral_connection,
    p_lateral_connection_decay=zero_95_exp_decay,
    num_lateral_connection_tries_per_unit=num_lateral_connection_tries_per_unit,
    learning_rate=learning_rate,
    loss=tf.keras.losses.CategoricalHinge(),
    metrics=[tf.keras.metrics.BinaryAccuracy(),
             tf.keras.metrics.Precision(),
             tf.keras.metrics.Recall()],
    epochs=epochs,
    project_name=f"{PROJECT_NAME}_meta_{meta_trial_number}",
    model_graphs='model_graphs',
    batch_size=batch_size,
    meta_trial_number=meta_trial_number,
    base_models=[tokenized_embedded_model],
    train_data_dtype=tf.string)

result = cerebros_automl.run_random_search()

print(f'Best accuracy achieved is {result}')
print(f'binary accuracy')

"""### Testing the best model found"""

#
# Load the best model (taking into account that it has a custom layer)
#
best_model_found =\
tf.keras.models.load_model(cerebros_automl.best_model_path,\
custom_objects={'GPT2Layer': GPT2Layer(max_seq_length)})

print('Evaluating on the test dataset')
best_model_found.evaluate(X_test, y_test)
